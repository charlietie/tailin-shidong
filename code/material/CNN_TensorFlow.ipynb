{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cifar10/cifar10-convnet-2929\n",
      "Average loss at step 2940: 51.2\n",
      "Average loss at step 2950: 12.5\n",
      "Average loss at step 2960: 31.0\n",
      "Average loss at step 2970: 61.2\n",
      "Average loss at step 2980:  8.2\n",
      "Average loss at step 2990: 38.7\n",
      "Average loss at step 3000:  9.7\n",
      "Average loss at step 3010: 61.1\n",
      "Average loss at step 3020: 46.0\n",
      "Average loss at step 3030: 11.4\n",
      "Average loss at step 3040: 18.3\n",
      "Average loss at step 3050: 12.7\n",
      "Average loss at step 3060: 24.3\n",
      "Average loss at step 3070: 46.4\n",
      "Average loss at step 3080:  7.0\n",
      "Average loss at step 3090:  2.5\n",
      "Average loss at step 3100:  8.3\n",
      "Average loss at step 3110: 22.4\n",
      "Average loss at step 3120: 13.8\n",
      "Average loss at step 3130: 31.5\n",
      "Average loss at step 3140: 10.1\n",
      "Average loss at step 3150: 63.5\n",
      "Average loss at step 3160: 31.5\n",
      "Average loss at step 3170:  2.7\n",
      "Average loss at step 3180: 18.2\n",
      "Average loss at step 3190: 87.4\n",
      "Average loss at step 3200: 38.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1870584a060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         _, loss_batch, summary = sess.run([optimizer, cross_entropy_loss, summary_op],\n\u001b[0;32m--> 155\u001b[0;31m                                           feed_dict={X: X_batch, Y: Y_batch, dropout: DROPOUT})\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\"\"\" Using convolutional net on CIFAR10 dataset\n",
    "One conv layer and one fc layer\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.keras.datasets.cifar10 import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import utils\n",
    "from utils import get_batch\n",
    "import tensorflow as tf\n",
    "\n",
    "N_CLASSES = 10  # there are only 10 classes in cifar10 dataset\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load Cifar10 data to the folder data/mnist\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "# One-hot encode the label for input\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Step 2: Define parameters for the model\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the CIFAR10 dataset is of shape 32*32*3 = 1024*3 = 3072\n",
    "# therefore, each image is represented with a [1,32,32,3] tensor\n",
    "# We'll be doing dropout for hidden layer so we'll need a placeholder\n",
    "# for the dropout probability too\n",
    "# Use None for shape so we can change the batch_size once we've built the graph\n",
    "\n",
    "with tf.name_scope('Input_data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3], 'X_placeholder')\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "# Step 4 + 5: create weights + do inference\n",
    "\"\"\"\n",
    "Step 4&5 : Create weights and do inference\n",
    "Model: conv1 -> relu -> pool -> fully connected -> softmax\n",
    "Loss: cross entropy and L2 loss\n",
    "\"\"\"\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    tf.summary.image(name='input_image', tensor=X, max_outputs=6)\n",
    "    kernel = tf.get_variable('kernel', [5, 5, 3, 32],\n",
    "                             initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [32],\n",
    "                             initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(X, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 28 x 28 x 32\n",
    "    # conv1 = layers.conv2d(X, 32, 5, 3, activation_fn=tf.nn.relu, padding='SAME')\n",
    "\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='SAME')\n",
    "\n",
    "with tf.variable_scope('fc') as scope:  # use weight of dimension 16*16*32 x 1024\n",
    "    input_features = 16 * 16 * 32\n",
    "    w = tf.get_variable('weights', [input_features, 1024],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [1024],\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "    reshaped_pool1 = tf.reshape(pool1, [-1, input_features])\n",
    "    fc = tf.nn.relu(tf.add(tf.matmul(reshaped_pool1, w), b, name='Wx_plus_b'), name='relu')\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    w = tf.get_variable('weights', [1024, N_CLASSES],\n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "    b = tf.get_variable('biases', [N_CLASSES],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "\n",
    "    logits = tf.matmul(fc, w) + b\n",
    "\n",
    "with tf.variable_scope('prediction_while_training'):\n",
    "    train_preds_op = tf.argmax(logits, axis=1)\n",
    "\n",
    "with tf.name_scope('cross_entropy_loss'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)\n",
    "    cross_entropy_loss = tf.reduce_mean(entropy, name='cross_entropy_loss')\n",
    "\n",
    "with tf.name_scope('l2_loss'):\n",
    "    pass\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    # This if for showing the test output image\n",
    "    output_image = tf.slice(input_=conv1, begin=[0, 0, 0, 0], size=[-1, -1, -1, 3], name='slice')\n",
    "    tf.summary.image('output_image', output_image, max_outputs=6)    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    \n",
    "with tf.name_scope('cifarNet'):\n",
    "    cifarNet_index = tf.argmax(logits, axis=1)\n",
    "    \n",
    "    \n",
    "# Define training operation\n",
    "# using gradient descent with learning rate of 0.001 to minimize cost\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(loss=cross_entropy_loss,\n",
    "                                                                         global_step=global_step)\n",
    "# ckpts dir\n",
    "utils.make_dir('checkpoints')\n",
    "utils.make_dir('checkpoints/cifar10')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # To visualize using  TensorBoard\n",
    "    writer = tf.summary.FileWriter('./graphs/cifar10', sess.graph)\n",
    "    # to start the tensorboard\n",
    "    # \" tensorboard --logdir=graphs/convnet/ \" at terminal\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/cifar10/checkpoint'))\n",
    "\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    initial_step = global_step.eval()  # global_step.eval() is \"0\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    n_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
    "    batch_flag = 0\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS):  # train the model N_EPOCH times\n",
    "\n",
    "        X_batch, Y_batch = get_batch(X_train, y_train, batch_size=BATCH_SIZE, index=batch_flag)\n",
    "\n",
    "        # control the data reader\n",
    "        if batch_flag >= n_batches:\n",
    "            batch_flag = 0\n",
    "        batch_flag += 1\n",
    "\n",
    "        _, loss_batch, summary = sess.run([optimizer, cross_entropy_loss, summary_op],\n",
    "                                          feed_dict={X: X_batch, Y: Y_batch, dropout: DROPOUT})\n",
    "\n",
    "        writer.add_summary(summary,global_step=index)\n",
    "        total_loss += loss_batch\n",
    "\n",
    "        if (index+1)%SKIP_STEP ==0:\n",
    "            print(\"Average loss at step {}:{:5.1f}\".format(index+1,total_loss/SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/cifar10/cifar10-convnet', index)\n",
    "\n",
    "    print(\"Optimization Finished\")#\n",
    "    print(\"Total time: {0} seconds\".format(time.time()-start_time))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cifar10/cifar10-convnet-3199\n",
      "output is : 9\n",
      "Label is : truck\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/cifar10/checkpoint'))\n",
    "    \n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    image_path = \"hotpot.jpg\"\n",
    "\n",
    "    img = image.load_img(image_path, target_size=(32,32))\n",
    "    \n",
    "    imgArray = image.img_to_array(img)\n",
    "    imgArray_ex = np.expand_dims(imgArray, axis=0)\n",
    "    modelInput = preprocess_input(imgArray_ex)\n",
    "    \n",
    "    predict = sess.run(train_preds_op,feed_dict={X:modelInput,dropout:0.9})\n",
    "    foo = int(predict)\n",
    "    \n",
    "    cifar10_labels={0:\"airplane\",\n",
    "             1:\"automobile\",\n",
    "             2:\"bird\",\n",
    "             3:\"cat\",\n",
    "             4:\"deer\",\n",
    "             5:\"dog\",\n",
    "             6:\"frog\",\n",
    "             7:\"horse\",\n",
    "             8:\"ship\",\n",
    "             9:\"truck\"}\n",
    "    print(\"output is :\",foo)\n",
    "    print(\"Label is :\", cifar10_labels[foo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
